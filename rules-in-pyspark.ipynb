{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2022-02-17T02:53:11.474845Z","iopub.execute_input":"2022-02-17T02:53:11.475402Z","iopub.status.idle":"2022-02-17T02:54:01.375182Z","shell.execute_reply.started":"2022-02-17T02:53:11.475348Z","shell.execute_reply":"2022-02-17T02:54:01.373983Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\n\nsc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\nspark = SparkSession.builder.getOrCreate()\n\nfrom pyspark.sql.functions import *\n","metadata":{"execution":{"iopub.status.busy":"2022-02-17T02:59:34.267876Z","iopub.execute_input":"2022-02-17T02:59:34.268591Z","iopub.status.idle":"2022-02-17T02:59:34.276743Z","shell.execute_reply.started":"2022-02-17T02:59:34.268555Z","shell.execute_reply":"2022-02-17T02:59:34.275880Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"products_table = spark.read.parquet(\"../input/products-parquet/products_parquet\")\nsales_table = spark.read.parquet(\"../input/salesparquet/sales_parquet\")\nsellers_table = spark.read.parquet(\"../input/sellers-parquet/sellers_parquet\")\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-17T02:57:26.933071Z","iopub.execute_input":"2022-02-17T02:57:26.934234Z","iopub.status.idle":"2022-02-17T02:57:27.977047Z","shell.execute_reply.started":"2022-02-17T02:57:26.934190Z","shell.execute_reply":"2022-02-17T02:57:27.976089Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"products_table.show()\nsales_table.show()\nsellers_table.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T02:57:42.233851Z","iopub.execute_input":"2022-02-17T02:57:42.234636Z","iopub.status.idle":"2022-02-17T02:57:43.045635Z","shell.execute_reply.started":"2022-02-17T02:57:42.234581Z","shell.execute_reply":"2022-02-17T02:57:43.044755Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#   Print the number of orders\nprint(\"Number of Orders: {}\".format(sales_table.count()))\n\n#   Print the number of sellers\nprint(\"Number of sellers: {}\".format(sellers_table.count()))\n\n#   Print the number of products\nprint(\"Number of products: {}\".format(products_table.count()))","metadata":{"execution":{"iopub.status.busy":"2022-02-17T02:58:08.906494Z","iopub.execute_input":"2022-02-17T02:58:08.906845Z","iopub.status.idle":"2022-02-17T02:58:12.034810Z","shell.execute_reply.started":"2022-02-17T02:58:08.906811Z","shell.execute_reply":"2022-02-17T02:58:12.033920Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#   Output how many products have been actually sold at least once\nprint(\"Number of products sold at least once\")\nsales_table.agg(countDistinct(col(\"product_id\"))).show()\n\n#   Output which is the product that has been sold in more orders\nprint(\"Product present in more orders\")\nsales_table.groupBy(col(\"product_id\")).agg(\n    count(\"*\").alias(\"cnt\")).orderBy(col(\"cnt\").desc()).limit(1).show()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T02:59:41.120803Z","iopub.execute_input":"2022-02-17T02:59:41.121332Z","iopub.status.idle":"2022-02-17T02:59:51.903491Z","shell.execute_reply.started":"2022-02-17T02:59:41.121287Z","shell.execute_reply":"2022-02-17T02:59:51.902599Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"how many distinct products have been sold in each date","metadata":{}},{"cell_type":"code","source":"sales_table.groupby(col(\"date\")).agg(countDistinct(col(\"product_id\")).alias(\"distinct_products_sold\")).orderBy(\n    col(\"distinct_products_sold\").desc()).show()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T03:09:13.703972Z","iopub.execute_input":"2022-02-17T03:09:13.704258Z","iopub.status.idle":"2022-02-17T03:09:19.285165Z","shell.execute_reply.started":"2022-02-17T03:09:13.704230Z","shell.execute_reply":"2022-02-17T03:09:19.284178Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"What is the average revenue of the orders?”","metadata":{}},{"cell_type":"code","source":"print(sales_table.join(products_table, sales_table[\"product_id\"] == products_table[\"product_id\"], \"inner\").\n      agg(avg(products_table[\"price\"] * sales_table[\"num_pieces_sold\"])).show())","metadata":{"execution":{"iopub.status.busy":"2022-02-17T03:10:32.137181Z","iopub.execute_input":"2022-02-17T03:10:32.137606Z","iopub.status.idle":"2022-02-17T03:11:54.750811Z","shell.execute_reply.started":"2022-02-17T03:10:32.137557Z","shell.execute_reply":"2022-02-17T03:11:54.749971Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"key Salting","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import Row\nfrom pyspark.sql.types import IntegerType\n\n# Step 1 - Check and select the skewed keys \n# In this case we are retrieving the top 100 keys: these will be the only salted keys.\nresults = sales_table.groupby(sales_table[\"product_id\"]).count().sort(col(\"count\").desc()).limit(100).collect()\n\n# Step 2 - What we want to do is:\n#  a. Duplicate the entries that we have in the dimension table for the most common products, e.g.\n#       product_0 will become: product_0-1, product_0-2, product_0-3 and so on\n#  b. On the sales table, we are going to replace \"product_0\" with a random duplicate (e.g. some of them \n#     will be replaced with product_0-1, others with product_0-2, etc.)\n# Using the new \"salted\" key will unskew the join\n\n# Let's create a dataset to do the trick\nREPLICATION_FACTOR = 101\nl = []\nreplicated_products = []\nfor _r in results:\n    replicated_products.append(_r[\"product_id\"])\n    for _rep in range(0, REPLICATION_FACTOR):\n        l.append((_r[\"product_id\"], _rep))\nrdd = spark.sparkContext.parallelize(l)\nreplicated_df = rdd.map(lambda x: Row(product_id=x[0], replication=int(x[1])))\nreplicated_df = spark.createDataFrame(replicated_df)\n\n#   Step 3: Generate the salted key\nproducts_table = products_table.join(broadcast(replicated_df),\n                                     products_table[\"product_id\"] == replicated_df[\"product_id\"], \"left\"). \\\n    withColumn(\"salted_join_key\", when(replicated_df[\"replication\"].isNull(), products_table[\"product_id\"]).otherwise(\n    concat(replicated_df[\"product_id\"], lit(\"-\"), replicated_df[\"replication\"])))\n\nsales_table = sales_table.withColumn(\"salted_join_key\", when(sales_table[\"product_id\"].isin(replicated_products),\n                                                             concat(sales_table[\"product_id\"], lit(\"-\"),\n                                                                    round(rand() * (REPLICATION_FACTOR - 1), 0).cast(\n                                                                        IntegerType()))).otherwise(\n    sales_table[\"product_id\"]))\n\n#   Step 4: Finally let's do the join\nprint(sales_table.join(products_table, sales_table[\"salted_join_key\"] == products_table[\"salted_join_key\"],\n                       \"inner\").\n      agg(avg(products_table[\"price\"] * sales_table[\"num_pieces_sold\"])).show())\n\nprint(\"Ok\")","metadata":{"execution":{"iopub.status.busy":"2022-02-17T03:13:53.369882Z","iopub.execute_input":"2022-02-17T03:13:53.370514Z","iopub.status.idle":"2022-02-17T03:15:34.045145Z","shell.execute_reply.started":"2022-02-17T03:13:53.370474Z","shell.execute_reply":"2022-02-17T03:15:34.043982Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"#   (Note that Spark will probably broadcast the table anyway, unless we forbid it throug the configuration paramters)\nprint(sales_table.join(sellers_table, sales_table[\"seller_id\"] == sellers_table[\"seller_id\"], \"inner\").withColumn(\n    \"ratio\", sales_table[\"num_pieces_sold\"]/sellers_table[\"daily_target\"]\n).groupBy(sales_table[\"seller_id\"]).agg(avg(\"ratio\")).show())\n\n#   Correct way through broarcasting\nprint(sales_table.join(broadcast(sellers_table), sales_table[\"seller_id\"] == sellers_table[\"seller_id\"], \"inner\").withColumn(\n    \"ratio\", sales_table[\"num_pieces_sold\"]/sellers_table[\"daily_target\"]\n).groupBy(sales_table[\"seller_id\"]).agg(avg(\"ratio\")).show())","metadata":{"execution":{"iopub.status.busy":"2022-02-17T03:18:42.897110Z","iopub.execute_input":"2022-02-17T03:18:42.897444Z","iopub.status.idle":"2022-02-17T03:18:56.142305Z","shell.execute_reply.started":"2022-02-17T03:18:42.897411Z","shell.execute_reply":"2022-02-17T03:18:56.141261Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Who are the second most selling and the least selling persons (sellers) for each product? Who are those for the product with product_id = 0”.","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import Row, Window\n\n\n# Calcuate the number of pieces sold by each seller for each product\nsales_table = sales_table.groupby(col(\"product_id\"), col(\"seller_id\")). \\\n    agg(sum(\"num_pieces_sold\").alias(\"num_pieces_sold\"))\n\n# Create the window functions, one will sort ascending the other one descending. Partition by the product_id\n# and sort by the pieces sold\nwindow_desc = Window.partitionBy(col(\"product_id\")).orderBy(col(\"num_pieces_sold\").desc())\nwindow_asc = Window.partitionBy(col(\"product_id\")).orderBy(col(\"num_pieces_sold\").asc())\n\n# Create a Dense Rank (to avoid holes)\nsales_table = sales_table.withColumn(\"rank_asc\", dense_rank().over(window_asc)). \\\n    withColumn(\"rank_desc\", dense_rank().over(window_desc))\n\n# Get products that only have one row OR the products in which multiple sellers sold the same amount\n# (i.e. all the employees that ever sold the product, sold the same exact amount)\nsingle_seller = sales_table.where(col(\"rank_asc\") == col(\"rank_desc\")).select(\n    col(\"product_id\").alias(\"single_seller_product_id\"), col(\"seller_id\").alias(\"single_seller_seller_id\"),\n    lit(\"Only seller or multiple sellers with the same results\").alias(\"type\")\n)\n\n# Get the second top sellers\nsecond_seller = sales_table.where(col(\"rank_desc\") == 2).select(\n    col(\"product_id\").alias(\"second_seller_product_id\"), col(\"seller_id\").alias(\"second_seller_seller_id\"),\n    lit(\"Second top seller\").alias(\"type\")\n)\n\n# Get the least sellers and exclude those rows that are already included in the first piece\n# We also exclude the \"second top sellers\" that are also \"least sellers\"\nleast_seller = sales_table.where(col(\"rank_asc\") == 1).select(\n    col(\"product_id\"), col(\"seller_id\"),\n    lit(\"Least Seller\").alias(\"type\")\n).join(single_seller, (sales_table[\"seller_id\"] == single_seller[\"single_seller_seller_id\"]) & (\n        sales_table[\"product_id\"] == single_seller[\"single_seller_product_id\"]), \"left_anti\"). \\\n    join(second_seller, (sales_table[\"seller_id\"] == second_seller[\"second_seller_seller_id\"]) & (\n        sales_table[\"product_id\"] == second_seller[\"second_seller_product_id\"]), \"left_anti\")\n\n# Union all the pieces\nunion_table = least_seller.select(\n    col(\"product_id\"),\n    col(\"seller_id\"),\n    col(\"type\")\n).union(second_seller.select(\n    col(\"second_seller_product_id\").alias(\"product_id\"),\n    col(\"second_seller_seller_id\").alias(\"seller_id\"),\n    col(\"type\")\n)).union(single_seller.select(\n    col(\"single_seller_product_id\").alias(\"product_id\"),\n    col(\"single_seller_seller_id\").alias(\"seller_id\"),\n    col(\"type\")\n))\nunion_table.show()\n\n# Which are the second top seller and least seller of product 0?\nunion_table.where(col(\"product_id\") == 0).show()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T03:22:17.405792Z","iopub.execute_input":"2022-02-17T03:22:17.406143Z","iopub.status.idle":"2022-02-17T03:22:47.709128Z","shell.execute_reply.started":"2022-02-17T03:22:17.406105Z","shell.execute_reply":"2022-02-17T03:22:47.707977Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}